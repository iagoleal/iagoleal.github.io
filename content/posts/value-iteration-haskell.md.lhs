---
title: Code Iterations on Value Iteration
keywords: [dynamic-programming]
date: 2024-02-16
description:
suppress-bibliography: true
---

In [a previous post](/posts/automata-monads),
we explored how to use monads to express various kinds of finite automata
as instances of dynamics with context.
These monadic transitions, though, appear in disguise in many other areas of mathematics.

Today we are going in a similar exploration of _Decision Processes_,
a close cousin to Finite Automata from the distant field of Optimal Control.
This post is a mix and match of three older posts on this blog:
a (theoretical) tutorial on dynamic programming and two explorations in Haskell.

- [A Tale of Dynamic Programming](/posts/dynamic-programming)
- [Memoization via Representables](/posts/representable-memoize)
- [A Fistful of Automata](/posts/automata-monads)

Hence, I may reference them when in need of any concept.

> {-# LANGUAGE RecordWildCards #-}
> {-# LANGUAGE TypeFamilies #-}
> {-# LANGUAGE AllowAmbiguousTypes #-}
> import Data.Foldable          (foldlM)
> import Data.List              (unfoldr)
> import Data.Kind              (Type)

Decision Processes
==================

For us, a _decision process_ consists of a controllable dynamical system
where, at each time step, we choose to take some action.
This transitions the system into some uncertain future state,
represented as a monadic context over the states.
Each such action also incurs a cost --- modeled as a real number ---
and, to keep up with inflation,
we introduce a discount factor $\gamma \in [0, 1)$
representing how much future costs are worth in today's value.

> -- | Finite automaton with states `s`, actions `a`, and monadic context `m`.
> --   The type parameters `s` and `a` are assumed to represent finite sets.
> data Process m s a = Process
>   { next     :: s -> a -> m s    -- ^ Change state with a context.
>   , cost     :: s -> a -> Double -- ^ Cost of choosing an action.
>   , discount :: Double           -- ^ How much future costs are worth in today's value.
>   }

The standard example of this structure are _Markov Decision Process_,
where `m` is a probability distribution monad,
representing stochastic transitions.
We can, nevertheless, use any appropriate context in place of it
and all the theory will keep working.
Have fun trying to fit your favorite iterated problem into this framework!

In formal languages,
the equivalent to the action set is an alphabet `a`
and the way to control the dynamics is to read a list of tokens in `[a]`.

> run :: Monad m => Process m s a -> [a] -> s -> m s
> run Process{..} = flip (foldlM next)

For decision process, on the other hand,
we usually want to implement a _policy_
which decides the action to take based on the current state.
To keep up with all the uncertainty theme, we will allow the policies to return monads of actions.
Deterministic policies can always be represented as non-deterministic ones via `pure` / `return`.

> type Policy m s a = s -> m a

In case you find it strange to allow monadic actions,
let's think about some "concrete" cases where it might make sense.
When working with Markov Decision Processes, for example,
it is customary to have probability distributions over actions,
in order to better model navigating in an uncertain world.
Another example are stochastic games,
where the monad represents actions inaccessible to you,
but that can be executed by your adversaries (`mÌ€ = Reader Enemy`).
Then, it is useful to model policies taking into account what the enemy.

With a policy at hand,
one generally want to simulate the system's dynamics according to it.
We thus at start at some initial state and indefinitely ask the policy
what to do, generating an infinite walk on the state space.

> simulate :: Monad m => Process m s a -> (s -> m a) -> s -> [m s]
> simulate p policy s = iterateM follow (pure s)
>  where
>   follow x = policy x >>= next p x  -- One step in the dynamics
>
> iterateM f x = x : iterateM f (f =<< x)


Every policy has a cost
-----------------------

Up until now,
we've only considered the dynamics generated by a `Process`.
But what about its cost?
The function `cost` only tells us the cost of a single decision,
but it is only part of the story.
What we need to know is the total cost of following a policy.

> -- Turn monad into a continuation (?)
> class Monad m => Observable m r where
>   collapse :: (s -> r) -> m s -> r
> 
> instance Num r => Observable [] r where
>  collapse f xs = sum (fmap f xs)


Value Iteration
===============

The main topic in [dynamic programming](/posts/dynamic-programming)
is finding optimal policies for decision processes.
One of its main insights is that the total cost `v :: s -> Double`
of starting at an initial state and following an optimal policy thereafter,
satisfies a recursive relation called the _Bellman equation_:

$$
  \begin{array}{rl}
   v^\star(s) =
    \min\limits_{a} & \mathrm{cost}(s, a) + \gamma v^\star(s') \\
    \textrm{s.t.}  & s' = \mathrm{next}(s, a).
  \end{array}
$$

From now on, this post will be entirely about how to solve this equation.
For computational reasons, we will focus on processes where both the states and actions are finite.
Which, in our context, means that they are orderable
and that we can enumerate all their elements.
To ease our life later on,
we assume this enumeration to be strictly sorted,
which is always feasible for finite sets.

> class (Eq a, Ord a) => Finite a where
>   elems :: [a]   -- WARNING: required to be sorted

A useful property of `Finite` types is that
optimization can be achieved via sheer brute force.

> minimize :: (Finite a, Ord r) => (a -> r) -> r
> minimize f = minimum (fmap f elems)
>
> maximize :: (Finite a, Ord r) => (a -> r) -> r
> maximize f = maximum (fmap f elems)

We will solve the Bellman equation using a method called **Value Iteration**.
It works by viewing the recursive relation as the fixed point of a functional operator
and using standards tools (which we will shortly define) to solve it.
Let's thus write a function converting a decision process to such operator.

> bellman :: (Observable m r, Finite a, r ~ Double)
>         => Process m s a -> (s -> r) -> (s -> r)
> bellman p v s = minimize (totalCost p v s)
>
> totalCost :: (Observable m r, r ~ Double)
>           => Process m s a -> (s -> r) -> s -> a -> r
> totalCost Process {..} v s a = cost s a + discount * collapse v s'
>  where s' = next s a

For seasoned haskellers,
fixed points mean a function that is as useful as it is simple:

> fix :: (t -> t) -> t
> fix f = let x = f x in x

Unfortunately for us,
jumping too fast into it may mean falling prey
to the pitfall of partial functions.
Because, even though `fix . bellman` type checks correctly,
its result is an infinite loop.
The function `fix` only really shines when dealing with some kind of lazy structure,
or when the recursion has a base case.
How can we write a solver for the Bellman equation then?

Thankfully,
the folks working in Analysis have already solved our problem
with the more than famous[^banach-famous]
[Banach Fixed Point Theorem](https://en.wikipedia.org/wiki/Banach_fixed-point_theorem).
It states that in a metric space (somewhere we can measure distances),
iterating a function places us arbitrarily close to its fixed point.

Let's, thus, define a class for metric space
and an appropriate instance for finite functions using the uniform norm.

[^banach-famous]: Famous just for us mathematicians, of course.

> class Metric x where
>   dist :: x -> x -> Double
>
> instance (Finite s) => Metric (s -> Double) where
>  dist f g = maximum [abs (f s - g s) | s <- elems]

Time for the fixed point iteration method.
To calculate the solution, we start at any $x_0$
and produce a converging sequence by turning the fixed point equation into an update rule.

$$ x_{n + 1} = f(x_n). $$

Similarly to `fix`,
this will build a chain of compositions `f \circ f \circ f \circ f ...$.
The main difference is that Banach's theorem
says that we can stop it when the results become close enough.

> fixBanach :: Metric x => Double -> x -> (x -> x) -> x
> fixBanach tol v0 f =
>   let v = f v0
>   in case compare (dist v v0) tol of
>     LT -> v
>     _  -> fixBanach tol v f

In dynamic programming,
value iteration finds the optimal value function
by using the method above on `bellman`.

> valueIteration :: (Observable m r, Finite s, Finite a, r ~ Double)
>                => Double -> Process m s a -> (s -> Double)
> valueIteration tol p = fixBanach tol (const 0) (bellman p)

Ok folks, we've solved our problem. Good night for everyone.
Although... the method above feels painfully slow.

Where is tabulation?
====================

Dynamic programming known for its use of tabulating solutions
for small parts of a problem in order to speed up larger ones.
The method above, nevertheless, only iterates functions.

The main problem is that `bellman p v`
builds a closure that must evaluate `v`
whenever it is evaluated.
Since the fixed point is formed by many compositions of `bellman p`,
it has to go through a chain of minimizations everytime we wnat to evaluate it.
Now imagine how slow it is to go through all elements when calculating
the function's distance!

A [classical technique for memoization](/posts/representable-memoize)
involves substituting a function by a representable functor
because of its faster indexing.
Let's introduce the class of `Representable` functors.

> class Functor f => Representable f where
>  type Key f :: Type
>  tabulate   :: (Key f -> a) -> f a
>  index      :: f a          -> (Key f -> a)
> -- Class laws:
> -- index    . tabulate = id
> -- tabulate . index    = id

What the class means is that `f`
is isomorphic to a function type with a fixed domain `Key f`.
Hence, we can freely switch between them
whenever one is more appropriate than the other.

The trick to memoization in Haskell is to instead of evaluating a function,
we create a new function that indexes a data structure tabulating it.

> rep :: forall f a. Representable f => ((Key f -> a) -> (Key f -> a)) -> (Key f -> a) -> (Key f -> a)
> rep g = index @f . tabulate . g

Notice that, by the class laws, `rep g` is semantically equivalent to `g`.
Nevertheless, they have very different runtime behaviours.
With this method, we can rewrite `valueIteration` to use a representation instead of the function itself.

> valueIterationRep :: forall f m s a r
>                .  (Representable f, Observable m r, Finite s, Finite a, s ~ Key f, r ~ Double)
>                => Double -> Process m s a -> (s -> r)
> valueIterationRep tol p = fixBanach tol (const 0) op
>  where op = rep @f (bellman p)

What I like in the implementation above
is that the data structure used to tabulate
is completely orthogonal to the algorithm per se.
We can choose it based on the problem at hand
without having to change any of the algorithm's internals.
